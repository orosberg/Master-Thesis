{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from combat.pycombat import pycombat\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed = pd.read_csv(\"./Data/data files/imputed/df_protein_lumbar_imputed_75_5_etr.csv\")\n",
    "X = df_imputed.to_numpy()\n",
    "df_classes = pd.read_csv(\"./Data/data files/iNPH_data_protein_median.csv\", usecols=[\"Cortical_biopsy_grouping\", \"CSF_type\", \"TMT Set\"])\n",
    "#y = y[y[\"CSF_type\"] == \"V\"].drop(columns=[\"CSF_type\"])[\"Cortical_biopsy_grouping\"]#.to_numpy()#.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_combat(df, TMT_set_indices):\n",
    "    \"\"\"Run ComBat to reduce batch effect on a dataframe.\n",
    "\n",
    "    :param df: Dataframe that ComBat is runned on\n",
    "    :param TMT_set_indices: Labels of which TMT batch each row belongs to.\n",
    "    :return: DF with ComBat applied to it\n",
    "    \"\"\"    \n",
    "    return pycombat(df.T, TMT_set_indices).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(df, n_samples, std = 0.1):\n",
    "    \"\"\"Helper function for simple perturbation function. Adds gaussian noise to randomly sampled rows of the df passed as an argument.\n",
    "\n",
    "    :param df: df filtered on class\n",
    "    :param n_samples: Number of samples to perform random sampling on\n",
    "    :param std: Standard deviation. Defaults to 0.1.\n",
    "    :return: df with added perturbed samples\n",
    "    \"\"\"      \n",
    "    sampled_df = df.sample(n = n_samples, replace = True)\n",
    "    gaussian_noise = np.random.normal(0, std, size=sampled_df.shape)\n",
    "    return sampled_df + gaussian_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_perturbation(X, y, n_samples_per_class = None, std = 0.1):\n",
    "    \"\"\"Performs a simple perturbation by sampling random rows of classes and adds gaussian noise to them\n",
    "\n",
    "    :param X: Dataframe with samples to get perturbed\n",
    "    :param y: Class labels\n",
    "    :param n_samples_per_class: Decides how many samples per class that are going to be sampled, defaults to None. \n",
    "    If none -> all classes will get equal weight according to size of current largest class.\n",
    "    :param std: How much the noise can deviate from the mean (Standard dev.), defaults to 0.1\n",
    "    :return: A df with the perturbed samples. A list stating which row belongs to which class.\n",
    "    \"\"\"    \n",
    "    classes = y.value_counts()\n",
    "    if n_samples_per_class is None:\n",
    "        largest_class = classes.argmax()\n",
    "        largest_n_samples = classes.pop(largest_class)\n",
    "    else:\n",
    "        largest_n_samples = n_samples_per_class\n",
    "    df_list = []\n",
    "    y_new_classes = list(y)\n",
    "    for idx, n_samples in classes.items():\n",
    "        perturbed = add_noise(X[y == idx], largest_n_samples - n_samples, std=std)\n",
    "        df_list.append(perturbed)\n",
    "        y_new_classes = y_new_classes + [idx] * len(perturbed)\n",
    "    return pd.concat([X] + df_list, axis=0), y_new_classes#, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost = XGBClassifier()\n",
    "xgboost.name = \"XGBoost\"\n",
    "lr = LogisticRegression()\n",
    "lr.name = \"LogisticRegression\"\n",
    "rf = RandomForestClassifier()\n",
    "rf.name = \"RandomForest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_params = {\"min_child_weight\": [1, 5, 10],\n",
    "                  \"gamma\": [0.5, 1, 1,5, 2, 5],\n",
    "                  \"subsample\": [0.6, 0.8, 1.0],\n",
    "                  \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "                  \"max_depth\": [3, 4, 5],\n",
    "                  \"n_estimators\": [100, 500, 1000]}\n",
    "lr_params = [{\"penalty\": [\"l1\"],\n",
    "              \"C\": np.arange(0.2, 3.1, 0.2),\n",
    "              \"solver\": [\"saga\"],\n",
    "              \"multi_class\": [\"multinomial\"],\n",
    "              \"max_iter\": np.arange(1000, 10001, 2000)\n",
    "             }]\n",
    "rf_params = {\"n_estimators\": np.arange(100, 501, 100),\n",
    "             \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "             \"max_depth\": [None, 10, 20, 40],\n",
    "             \"max_features\": [\"sqrt\", \"log2\"],\n",
    "             \"min_samples_leaf\": [1, 2, 4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [xgboost, lr, rf]\n",
    "params = [xgboost_params, lr_params, rf_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_pt1(X, df_classes, csf_type, n_samples_per_class = 100, do_combat = True, do_std_scaler = True,):\n",
    "    type = df_classes[df_classes[\"CSF_type\"] == csf_type].reset_index(drop=True)\n",
    "    y = type[\"Cortical_biopsy_grouping\"]\n",
    "    tmt_set = type[\"TMT Set\"].values\n",
    "    if do_combat:\n",
    "        X = run_combat(X, tmt_set)\n",
    "    X, y = simple_perturbation(X, y, n_samples_per_class = n_samples_per_class)\n",
    "    if do_std_scaler:\n",
    "        std_scaler = StandardScaler()\n",
    "        X = std_scaler.fit_transform(X)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_pt2(X, y, models, params, cv = 5, n_jobs = -1):\n",
    "    best_params = {}\n",
    "    for model, param in zip(models, params):\n",
    "        start_time = time()\n",
    "        print(f\"{model.name} started.\")\n",
    "        clf = GridSearchCV(model, param_grid=param, cv=cv, n_jobs=n_jobs)\n",
    "        clf.fit(X, y)\n",
    "        best_params[model.name] = clf.best_params_\n",
    "        print(f\"{model.name} is done in {time() - start_time} seconds.\\n\" )\n",
    "    return best_params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 batches.\n",
      "Adjusting for 0 covariate(s) or covariate level(s).\n",
      "Standardizing Data across genes.\n",
      "Fitting L/S model and finding priors.\n",
      "Finding parametric adjustments.\n",
      "Adjusting the Data\n"
     ]
    }
   ],
   "source": [
    "X, y = pipeline_pt1(df_imputed, df_classes, \"L\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = pipeline_pt2(X, y, models, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 100, 0: 100, 2: 100})"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_pt2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_pt3(X, y, models, params, tmt_set, K = 5, n_samples_per_class = 100):\n",
    "    X = run_combat(X, tmt_set)\n",
    "    k_fold = KFold(n_splits=K, shuffle=True)\n",
    "    for i, (train_index, test_index) in enumerate(k_fold.split(X)):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        X_train, y_train = simple_perturbation(X_train, y_train, n_samples_per_class=n_samples_per_class)\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        for model in \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_fold = KFold(n_splits=5, shuffle=True)\n",
    "k_fold.get_n_splits(X=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Train: [ 0  1  2  3  4  5  6  7  9 10 11 13 16 17 21 23 24 25 26 27 28 29 30 31\n",
      " 32 33 34 37 38 39 41 42 44 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60\n",
      " 61 62 63 64 65 66 69 70 71 73 74 75 76 77 78 80 81 82 83 84]\n",
      "Test: [ 8 12 14 15 18 19 20 22 35 36 40 43 45 67 68 72 79]\n",
      "Fold: 1\n",
      "Train: [ 0  2  3  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 25 27 29\n",
      " 31 32 33 34 35 36 37 39 40 42 43 44 45 46 47 48 49 53 54 55 56 57 58 59\n",
      " 61 62 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 82 83]\n",
      "Test: [ 1  4  5 24 26 28 30 38 41 50 51 52 60 63 64 81 84]\n",
      "Fold: 2\n",
      "Train: [ 1  3  4  5  7  8  9 10 11 12 13 14 15 17 18 19 20 22 23 24 26 28 29 30\n",
      " 34 35 36 37 38 40 41 42 43 44 45 46 49 50 51 52 53 54 55 56 57 60 61 62\n",
      " 63 64 65 66 67 68 70 71 72 73 74 75 76 77 78 79 81 82 83 84]\n",
      "Test: [ 0  2  6 16 21 25 27 31 32 33 39 47 48 58 59 69 80]\n",
      "Fold: 3\n",
      "Train: [ 0  1  2  3  4  5  6  8 11 12 13 14 15 16 18 19 20 21 22 23 24 25 26 27\n",
      " 28 30 31 32 33 34 35 36 38 39 40 41 42 43 45 46 47 48 49 50 51 52 55 56\n",
      " 57 58 59 60 61 63 64 65 67 68 69 71 72 74 75 79 80 81 82 84]\n",
      "Test: [ 7  9 10 17 29 37 44 53 54 62 66 70 73 76 77 78 83]\n",
      "Fold: 4\n",
      "Train: [ 0  1  2  4  5  6  7  8  9 10 12 14 15 16 17 18 19 20 21 22 24 25 26 27\n",
      " 28 29 30 31 32 33 35 36 37 38 39 40 41 43 44 45 47 48 50 51 52 53 54 58\n",
      " 59 60 62 63 64 66 67 68 69 70 72 73 76 77 78 79 80 81 83 84]\n",
      "Test: [ 3 11 13 23 34 42 46 49 55 56 57 61 65 71 74 75 82]\n"
     ]
    }
   ],
   "source": [
    "for i, (train_index, test_index) in enumerate(k_fold.split(X)):\n",
    "    print(f'Fold: {i}')\n",
    "    print(f'Train: {train_index}')\n",
    "    print(f'Test: {test_index}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 11, 13, 23, 34, 42, 46, 49, 55, 56, 57, 61, 65, 71, 74, 75, 82])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
