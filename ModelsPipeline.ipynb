{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from combat.pycombat import pycombat\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed = pd.read_csv(\"./Data/data files/imputed/df_protein_lumbar_imputed_75_5_etr.csv\")\n",
    "X = df_imputed.to_numpy()\n",
    "df_classes = pd.read_csv(\"./Data/data files/iNPH_data_protein_median.csv\", usecols=[\"Cortical_biopsy_grouping\", \"CSF_type\", \"TMT Set\"])\n",
    "#y = y[y[\"CSF_type\"] == \"V\"].drop(columns=[\"CSF_type\"])[\"Cortical_biopsy_grouping\"]#.to_numpy()#.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_combat(df, TMT_set_indices):\n",
    "    \"\"\"Run ComBat to reduce batch effect on a dataframe.\n",
    "\n",
    "    :param df: Dataframe that ComBat is runned on\n",
    "    :param TMT_set_indices: Labels of which TMT batch each row belongs to.\n",
    "    :return: DF with ComBat applied to it\n",
    "    \"\"\"    \n",
    "    return pycombat(df.T, TMT_set_indices).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(df, n_samples, std = 0.1):\n",
    "    \"\"\"Helper function for simple perturbation function. Adds gaussian noise to randomly sampled rows of the df passed as an argument.\n",
    "\n",
    "    :param df: df filtered on class\n",
    "    :param n_samples: Number of samples to perform random sampling on\n",
    "    :param std: Standard deviation. Defaults to 0.1.\n",
    "    :return: df with added perturbed samples\n",
    "    \"\"\"      \n",
    "    sampled_df = df.sample(n = n_samples, replace = True)\n",
    "    gaussian_noise = np.random.normal(0, std, size=sampled_df.shape)\n",
    "    return sampled_df + gaussian_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_perturbation(X, y, n_samples_per_class = None, std = 0.1):\n",
    "    \"\"\"Performs a simple perturbation by sampling random rows of classes and adds gaussian noise to them\n",
    "\n",
    "    :param X: Dataframe with samples to get perturbed\n",
    "    :param y: Class labels\n",
    "    :param n_samples_per_class: Decides how many samples per class that are going to be sampled, defaults to None. \n",
    "    If none -> all classes will get equal weight according to size of current largest class.\n",
    "    :param std: How much the noise can deviate from the mean (Standard dev.), defaults to 0.1\n",
    "    :return: A df with the perturbed samples. A list stating which row belongs to which class.\n",
    "    \"\"\"    \n",
    "    classes = y.value_counts()\n",
    "    if n_samples_per_class is None:\n",
    "        largest_class = classes.argmax()\n",
    "        largest_n_samples = classes.pop(largest_class)\n",
    "    else:\n",
    "        largest_n_samples = n_samples_per_class\n",
    "    df_list = []\n",
    "    y_new_classes = list(y)\n",
    "    for idx, n_samples in classes.items():\n",
    "        perturbed = add_noise(X[y == idx], largest_n_samples - n_samples, std=std)\n",
    "        df_list.append(perturbed)\n",
    "        y_new_classes = y_new_classes + [idx] * len(perturbed)\n",
    "    return pd.concat([X] + df_list, axis=0), y_new_classes#, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost = XGBClassifier()\n",
    "xgboost.name = \"XGBoost\"\n",
    "lr = LogisticRegression()\n",
    "lr.name = \"LogisticRegression\"\n",
    "rf = RandomForestClassifier()\n",
    "rf.name = \"RandomForest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_params = {\"min_child_weight\": [1, 5, 10],\n",
    "                  \"gamma\": [0.5, 1, 1,5, 2, 5],\n",
    "                  \"subsample\": [0.6, 0.8, 1.0],\n",
    "                  \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "                  \"max_depth\": [3, 4, 5],\n",
    "                  \"n_estimators\": [100, 500, 1000]}\n",
    "lr_params = [{\"penalty\": [\"l1\"],\n",
    "              \"C\": np.arange(0.2, 3.1, 0.2),\n",
    "              \"solver\": [\"saga\"],\n",
    "              \"multi_class\": [\"multinomial\"],\n",
    "              \"max_iter\": np.arange(1000, 10001, 2000)\n",
    "             }]\n",
    "rf_params = {\"n_estimators\": np.arange(100, 501, 100),\n",
    "             \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "             \"max_depth\": [None, 10, 20, 40],\n",
    "             \"max_features\": [\"sqrt\", \"log2\"],\n",
    "             \"min_samples_leaf\": [1, 2, 4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [xgboost, lr, rf]\n",
    "params = [xgboost_params, lr_params, rf_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_pt1(X, df_classes, csf_type, n_samples_per_class = 100, do_combat = True, do_std_scaler = True,):\n",
    "    type = df_classes[df_classes[\"CSF_type\"] == csf_type].reset_index(drop=True)\n",
    "    y = type[\"Cortical_biopsy_grouping\"]\n",
    "    tmt_set = type[\"TMT Set\"].values\n",
    "    if do_combat:\n",
    "        X = run_combat(X, tmt_set)\n",
    "    X, y = simple_perturbation(X, y, n_samples_per_class = n_samples_per_class, std=1)\n",
    "    if do_std_scaler:\n",
    "        std_scaler = StandardScaler()\n",
    "        X = std_scaler.fit_transform(X)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_pt2(X, y, models, params, cv = 5, n_jobs = -1):\n",
    "    best_params = {}\n",
    "    for model, param in zip(models, params):\n",
    "        start_time = time()\n",
    "        print(f\"{model.name} started.\")\n",
    "        clf = GridSearchCV(model, param_grid=param, cv=cv, n_jobs=n_jobs)\n",
    "        clf.fit(X, y)\n",
    "        best_params[model.name] = clf.best_params_\n",
    "        print(f\"{model.name} is done in {time() - start_time} seconds.\\n\" )\n",
    "    return best_params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 batches.\n",
      "Adjusting for 0 covariate(s) or covariate level(s).\n",
      "Standardizing Data across genes.\n",
      "Fitting L/S model and finding priors.\n",
      "Finding parametric adjustments.\n",
      "Adjusting the Data\n"
     ]
    }
   ],
   "source": [
    "dfx, dfy = pipeline_pt1(df_imputed, df_classes, \"L\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = pipeline_pt2(dfx, dfy, models, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 100, 0: 100, 2: 100})"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_pt2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
